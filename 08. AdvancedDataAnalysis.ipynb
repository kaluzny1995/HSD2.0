{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Speech Detector 2.0\n",
    "---\n",
    "**Advanced data analysis**\n",
    "1. Calculation of **phrase occurence** in text:\n",
    "    1. Does the phrase occur fully or partially, how?\n",
    "    2. **POC - Phrase Occurence Coefficient** - Get max, mean and min values.\n",
    "    3. 1.0 means full hate speech --> 0.0 mean no hate speech\n",
    "    4. Visualization of POC calculation examples\n",
    "2. For each of 7 hate-speech classes and one vulgar:\n",
    "    1. Load of appropriate .txt file with dictionary with lemmatized hateful phrases\n",
    "    2. For each lemmatized tweet:\n",
    "        1. Calculate min, mean and max **POC** scores, according to appropriate **hateful or vulgar phrases**.\n",
    "        2. Get average values of mins, means and maxes.\n",
    "    3. Save results into .csv file.\n",
    "3. Polish polyglot sentiment analysis\n",
    "4. Characters, syllables, words counting.\n",
    "5. For each of 7 hate speech classes and one vulgar:\n",
    "    1. Detect N hateful topics which include K words. (assume N and K values)\n",
    "    2. Save **LDA (Latent Dirichlet Allocation)** model.\n",
    "    3. For each lemmatized tweet:\n",
    "        1. Calculate **POC** scores of **topics** (treating them as phrases) and mean aggregate over topics.\n",
    "    4. Save results into .csv file.\n",
    "6. For each tweet:\n",
    "    1. Determine how many words have which type of **polyglot sentiment**.\n",
    "    2. Count characters, syllables, words and unique words.\n",
    "    3. Compare polyglot sentiment results with empirical sentiment annotations. Calculate accuracy and F measures.\n",
    "    4. Save results into .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "from src.extension.lemm import lemmatize_text\n",
    "from src.measures import POC\n",
    "from src.utils.lemm import load_lemmatized_tweets, load_lemm_phrases\n",
    "from src.utils.ext import load_ext_phrases\n",
    "from src.analysis.poc import analyse_POC\n",
    "from src.analysis.lda import train_lda_models\n",
    "from src.analysis.topic_poc import analyse_topic_POC\n",
    "from src.analysis.other import analyse_other\n",
    "from src.utils.texts import text_sentiment, text_numbers\n",
    "from src.constants import (POC_SCORES_PATH, TOPIC_POC_SCORES_PATH, OTHER_SCORES_PATH,\n",
    "                           POLISH_STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrases occurance calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to calculate phrase occurence coefficient (POC) in text?**\n",
    "\n",
    "1. Split by whitespace lemmatized text and phrase to separate words.\n",
    "2. Delete all stopwords and interpunction symbols from text and phrase.\n",
    "3. Enumerate all words left in text, starting from 0.\n",
    "3. For each word in phrase list all occurences (i.e. referring numbers) of the word in text. If no occurences of word in text found, then omit it (empty list).\n",
    "4. Get all possible phrase words orders in examined text i.e. perform cartesian product for positions lists.\n",
    "5. For each possible order:\n",
    "    1. Form n list of occurences into n-1 pairs.\n",
    "    2. For each pair assign (1) if first element is smaller than second (ascending order) else (-1)\n",
    "    3. Sum all assignations and divide the total by number of pairs (i.e. words in phrase - 1).\n",
    "6. Return minimum, mean and maximum score.\n",
    "\n",
    "---\n",
    "**EXAMPLE 1.**:<br />\n",
    "**text**: *Wróciły pisowskie trójki sądy doraźne koksowniki i SKOTy, do tego PiS gwałci żeby nie robić aborcji* <br />\n",
    "**phrase**: *PiS gwałci żeby nie robić aborcji*<br />\n",
    "![schema 01](charts/schemes/HSD2.0_scheme01.png)<br />\n",
    "Results: **MIN=1.0 MEAN=1.0 MAX=1.0**\n",
    "\n",
    "---\n",
    "**EXAMPLE 2.**:<br />\n",
    "**text**: *Faszystowskie sądy ach faszystowskie sądy*<br/>\n",
    "**phrase** : *Ach faszystowskie sądy fałszywe*<br />\n",
    "![schema 02](charts/schemes/HSD2.0_scheme02.png)<br />\n",
    "Results: **MIN=-0.5 MEAN=0.25 MAX=0.5**\n",
    "\n",
    "---\n",
    "**EXAMPLE 3.**:<br />\n",
    "**text**: *Ci z LGBT chcą zniszczyć pojęcia tradycji rodziny tworzonej przez mężczyznę i kobietę!*<br/>\n",
    "**phrase**: *LGBT zniszczą nową rodziny tradycję mężczyzn i kobiet.*<br />\n",
    "![schema 03](charts/schemes/HSD2.0_scheme03.png)<br />\n",
    "Results: **MIN=0.17 MEAN=0.33 MAX=0.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0, 1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Wróciły pisowskie trójki sądy doraźne koksowniki i SKOTy, do tego PiS gwałci żeby nie robić aborcji'\n",
    "phrase = 'PiS gwałci żeby nie robić aborcji'\n",
    "\n",
    "POC(text, phrase, stopwords=POLISH_STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 0.25, 0.5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Faszystowskie sądy ach faszystowskie sądy'\n",
    "phrase = 'Ach faszystowskie sądy fałszywe'\n",
    "\n",
    "POC(text, phrase, stopwords=POLISH_STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 0.5, 0.5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Ci z LGBT chcą zniszczyć pojęcia tradycji rodziny tworzonej przez mężczyznę i kobietę!'\n",
    "phrase = 'LGBT zniszczą nową rodziny tradycję mężczyzn i kobiet.'\n",
    "\n",
    "POC(text, phrase, stopwords=POLISH_STOPWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>w czwartek muszę poprawić sądy i trybunały</td>\n",
       "      <td>w czwartek musieć poprawić sąd i trybunał</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>Żale Nałęcza i riposta Macierewicza: Pan był w kompartii, czy ma prawo wy­gła­szać takie sądy? | niezalezna.pl</td>\n",
       "      <td>żale nałęcz i riposta macierewicz pan być w kompartia czy mieć prawo wyżgłaćszać taki sąd niezalezna.pl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  \\\n",
       "0   9   \n",
       "1   8   \n",
       "\n",
       "                                                                                                             tweet  \\\n",
       "0                                                                       w czwartek muszę poprawić sądy i trybunały   \n",
       "1  Żale Nałęcza i riposta Macierewicza: Pan był w kompartii, czy ma prawo wy­gła­szać takie sądy? | niezalezna.pl    \n",
       "\n",
       "                                                                                                lemmatized  \n",
       "0                                                                w czwartek musieć poprawić sąd i trybunał  \n",
       "1  żale nałęcz i riposta macierewicz pan być w kompartia czy mieć prawo wyżgłaćszać taki sąd niezalezna.pl  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_lemmatized_tweets()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakub/Dokumenty/Semestr 9.2/HSD2/src/utils/lemm.py:33: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(aphr)\n"
     ]
    }
   ],
   "source": [
    "lemm_phrases = load_lemm_phrases(load_vulg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakub/Dokumenty/Semestr 9.2/HSD2/src/utils/ext.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(aphr)\n"
     ]
    }
   ],
   "source": [
    "ext_phrases = load_ext_phrases(load_vulg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate POC score for all tweets.**\n",
    "\n",
    "1. Load relevant data with sanitized tweets and all hateful phrases.\n",
    "2. For each tweet:\n",
    "    1. For each hate type (and one vulgar):\n",
    "        1. Calculate POC scores (min, mean, max) for every phrase which belongs to certain hate type (or vulgar)\n",
    "        2. Get means of minimum, mean and maximum POC scores\n",
    "        3. Write calculations into dictionary\n",
    "    2. Write all hate types dictionary values into .csv row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(POC_SCORES_PATH):\n",
    "    analyse_POC(df, ext_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>wyz_POC_min</th>\n",
       "      <th>wyz_POC_mean</th>\n",
       "      <th>wyz_POC_max</th>\n",
       "      <th>groz_POC_min</th>\n",
       "      <th>groz_POC_mean</th>\n",
       "      <th>groz_POC_max</th>\n",
       "      <th>wyk_POC_min</th>\n",
       "      <th>wyk_POC_mean</th>\n",
       "      <th>wyk_POC_max</th>\n",
       "      <th>...</th>\n",
       "      <th>pon_POC_max</th>\n",
       "      <th>styg_POC_min</th>\n",
       "      <th>styg_POC_mean</th>\n",
       "      <th>styg_POC_max</th>\n",
       "      <th>szan_POC_min</th>\n",
       "      <th>szan_POC_mean</th>\n",
       "      <th>szan_POC_max</th>\n",
       "      <th>vulg_POC_min</th>\n",
       "      <th>vulg_POC_mean</th>\n",
       "      <th>vulg_POC_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.002193</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.004526</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006219</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.004606</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  wyz_POC_min  wyz_POC_mean  wyz_POC_max  groz_POC_min  groz_POC_mean  \\\n",
       "0   9     0.000000      0.000000          0.0          -0.5      -0.002193   \n",
       "1   8    -0.333333      0.004526          0.5          -0.5       0.000808   \n",
       "\n",
       "   groz_POC_max  wyk_POC_min  wyk_POC_mean  wyk_POC_max  ...  pon_POC_max  \\\n",
       "0           0.5          0.0      0.000000     0.000000  ...          0.5   \n",
       "1           0.5          0.0      0.006219     0.333333  ...          0.5   \n",
       "\n",
       "   styg_POC_min  styg_POC_mean  styg_POC_max  szan_POC_min  szan_POC_mean  \\\n",
       "0          -0.5       0.000260      0.500000           0.0            0.0   \n",
       "1          -0.5      -0.004606      0.333333           0.0            0.0   \n",
       "\n",
       "   szan_POC_max  vulg_POC_min  vulg_POC_mean  vulg_POC_max  \n",
       "0           0.0           0.0            0.0           0.0  \n",
       "1           0.0           0.0            0.0           0.0  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_poc_scores = pd.read_csv(POC_SCORES_PATH)\n",
    "df_poc_scores.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hateful phrases topics detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find top 20 topic 20-words sentences for phrases of each hate type (and one vulgar).**\n",
    "\n",
    "1. For each hate type:\n",
    "    1. Get relevant extended phrases.\n",
    "    2. Fit CountVectorizer and LDA model.\n",
    "    3. Save trained model into pickle archive.\n",
    "    4. For each tweet:\n",
    "        1. Calculate POC scores of each of 20 topics appearance.\n",
    "        2. Save into .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_N_TOPICS, LDA_N_WORDS = 20, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e9c41352834d8280da7b0b642bc5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_lda_models(ext_phrases, n_topics=LDA_N_TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(TOPIC_POC_SCORES_PATH):\n",
    "    analyse_topic_POC(df, n_words=LDA_N_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>wyz_topic_POC_min</th>\n",
       "      <th>wyz_topic_POC_mean</th>\n",
       "      <th>wyz_topic_POC_max</th>\n",
       "      <th>groz_topic_POC_min</th>\n",
       "      <th>groz_topic_POC_mean</th>\n",
       "      <th>groz_topic_POC_max</th>\n",
       "      <th>wyk_topic_POC_min</th>\n",
       "      <th>wyk_topic_POC_mean</th>\n",
       "      <th>wyk_topic_POC_max</th>\n",
       "      <th>...</th>\n",
       "      <th>pon_topic_POC_max</th>\n",
       "      <th>styg_topic_POC_min</th>\n",
       "      <th>styg_topic_POC_mean</th>\n",
       "      <th>styg_topic_POC_max</th>\n",
       "      <th>szan_topic_POC_min</th>\n",
       "      <th>szan_topic_POC_mean</th>\n",
       "      <th>szan_topic_POC_max</th>\n",
       "      <th>vulg_topic_POC_min</th>\n",
       "      <th>vulg_topic_POC_mean</th>\n",
       "      <th>vulg_topic_POC_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.010526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.002632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.010526</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  wyz_topic_POC_min  wyz_topic_POC_mean  wyz_topic_POC_max  \\\n",
       "0   9                0.0            0.000000           0.000000   \n",
       "1   8                0.0            0.005263           0.052632   \n",
       "\n",
       "   groz_topic_POC_min  groz_topic_POC_mean  groz_topic_POC_max  \\\n",
       "0           -0.052632             0.000000            0.052632   \n",
       "1           -0.052632            -0.010526            0.000000   \n",
       "\n",
       "   wyk_topic_POC_min  wyk_topic_POC_mean  wyk_topic_POC_max  ...  \\\n",
       "0           0.000000            0.000000           0.000000  ...   \n",
       "1          -0.052632           -0.002632           0.052632  ...   \n",
       "\n",
       "   pon_topic_POC_max  styg_topic_POC_min  styg_topic_POC_mean  \\\n",
       "0           0.052632           -0.052632             0.002632   \n",
       "1           0.000000           -0.052632            -0.010526   \n",
       "\n",
       "   styg_topic_POC_max  szan_topic_POC_min  szan_topic_POC_mean  \\\n",
       "0            0.052632                 0.0                  0.0   \n",
       "1            0.052632                 0.0                  0.0   \n",
       "\n",
       "   szan_topic_POC_max  vulg_topic_POC_min  vulg_topic_POC_mean  \\\n",
       "0                 0.0                 0.0                  0.0   \n",
       "1                 0.0                 0.0                  0.0   \n",
       "\n",
       "   vulg_topic_POC_max  \n",
       "0                 0.0  \n",
       "1                 0.0  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_poc_scores = pd.read_csv(TOPIC_POC_SCORES_PATH)\n",
    "df_topic_poc_scores.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other text scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polish Polyglot sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 17, 0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sentiment('Wróciły pisowskie trójki sądy doraźne koksowniki i SKOTy, do tego PiS gwałci żeby nie robić aborcji')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5, 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sentiment('Faszystowskie sądy ach faszystowskie sądy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 12, 0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sentiment('Ci z LGBT chcą zniszczyć pojęcia tradycji rodziny tworzonej przez mężczyznę i kobietę!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characters, syllables, words counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 33, 16, 16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_numbers('Wróciły pisowskie trójki sądy doraźne koksowniki i SKOTy, do tego PiS gwałci żeby nie robić aborcji')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 13, 5, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_numbers('Faszystowskie sądy ach faszystowskie sądy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74, 26, 13, 13)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_numbers('Ci z LGBT chcą zniszczyć pojęcia tradycji rodziny tworzonej przez mężczyznę i kobietę!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78, 29, 16, 16)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_numbers(lemmatize_text('Wróciły pisowskie trójki sądy doraźne koksowniki i SKOTy, do tego PiS gwałci żeby nie robić aborcji'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 11, 5, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_numbers(lemmatize_text('Faszystowskie sądy ach faszystowskie sądy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74, 25, 13, 13)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_numbers(lemmatize_text('Ci z LGBT chcą zniszczyć pojęcia tradycji rodziny tworzonej przez mężczyznę i kobietę!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate above other scores for all tweets.**\n",
    "\n",
    "1. Load relevant data with sanitized tweets.\n",
    "2. For each tweet:\n",
    "    1. Remove invalid (for polyglot) characters which cause errors.\n",
    "    2. Determine how many words have which of three sentiment types.\n",
    "    3. Count characters, syllables, words and unique words.\n",
    "    2. Write all values into .csv row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(OTHER_SCORES_PATH):\n",
    "    analyse_other(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>s_neg</th>\n",
       "      <th>s_neu</th>\n",
       "      <th>s_pos</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>n_sylls</th>\n",
       "      <th>n_words</th>\n",
       "      <th>nu_words</th>\n",
       "      <th>nl_chars</th>\n",
       "      <th>nl_sylls</th>\n",
       "      <th>nl_words</th>\n",
       "      <th>nlu_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>38</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>88</td>\n",
       "      <td>33</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  s_neg  s_neu  s_pos  n_chars  n_sylls  n_words  nu_words  nl_chars  \\\n",
       "0   9      0      6      1       36       15        7         7        35   \n",
       "1   8      1     18      1       94       38       18        18        88   \n",
       "\n",
       "   nl_sylls  nl_words  nlu_words  \n",
       "0        13         7          7  \n",
       "1        33        16         16  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_other_scores = pd.read_csv(OTHER_SCORES_PATH)\n",
    "df_other_scores.head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
