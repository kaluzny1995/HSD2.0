{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Speech Detector 2.0\n",
    "---\n",
    "**Advanced data analysis**\n",
    "1. Calculation of **phrase occurence** in text:\n",
    "    1. Does the phrase occur fully or partially, how?\n",
    "    2. **POC - Phrase Occurence Coefficient** - Get max, mean and min values.\n",
    "    3. 1.0 means full hate speech --> 0.0 mean no hate speech\n",
    "    4. Visualization of POC calculation examples\n",
    "2. For each of 7 hate-speech classes and one vulgar:\n",
    "    1. Load of appropriate .txt file with dictionary with lemmatized hateful phrases\n",
    "    2. For each lemmatized tweet:\n",
    "        1. Calculate min, mean and max **POC** scores, according to appropriate **hateful or vulgar phrases**.\n",
    "        2. Get average values of mins, means and maxes.\n",
    "    3. Save results into .csv file.\n",
    "3. Polish polyglot sentiment analysis\n",
    "4. Characters, syllables, words counting.\n",
    "5. For each of 7 hate speech classes and one vulgar:\n",
    "    1. Detect N hateful topics which include K words. (assume N and K values)\n",
    "    2. Save **LDA (Latent Dirichlet Allocation)** model.\n",
    "    3. For each lemmatized tweet:\n",
    "        1. Calculate **POC** scores of **topics** (treating them as phrases) and mean aggregate over topics.\n",
    "    4. Save results into .csv file.\n",
    "6. For each tweet:\n",
    "    1. Determine how many words have which type of **polyglot sentiment**.\n",
    "    2. Count characters, syllables, words and unique words.\n",
    "    3. Compare polyglot sentiment results with empirical sentiment annotations. Calculate accuracy and F measures.\n",
    "    4. Save results into .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import itertools\n",
    "\n",
    "from combo.predict import COMBO\n",
    "from polyglot.text import Text\n",
    "from polyglot.downloader import downloader\n",
    "\n",
    "import pyphen\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['wyzywanie', 'grożenie', 'wykluczanie', 'odczłowieczanie', 'poniżanie',\n",
    "          'stygmatyzacja', 'szantaż', 'wulgaryzm']\n",
    "LABELS_SMALL = ['wyz', 'groz', 'wyk', 'odcz', 'pon', 'styg', 'szan']\n",
    "LABELS_V_SMALL = LABELS_SMALL + ['vulg']\n",
    "\n",
    "DUPLICATED_PATH = 'data/tweets_sady/processed/sady_duplicated.csv'\n",
    "LEMMAS_PATH = 'data/tweets_sady/processed/lemmas.csv'\n",
    "POC_SCORES_PATH = 'data/tweets_sady/processed/poc_scores.csv'\n",
    "TOPIC_POC_SCORES_PATH = 'data/tweets_sady/processed/topic_poc_scores.csv'\n",
    "OTHER_SCORES_PATH = 'data/tweets_sady/processed/other_scores.csv'\n",
    "\n",
    "HATEFUL_LEMM_DIR = 'data/hateful/lemm_{}.txt'\n",
    "VULGARS_LEMM_DIR = 'data/vulgars/lemm_{}.txt'\n",
    "HATEFUL_EXT_DIR = 'data/hateful/ext_{}.txt'\n",
    "VULGARS_EXT_DIR = 'data/vulgars/ext_{}.txt'\n",
    "LDA_MODEL_DIR = 'models/lda/lda_{}.pkl'\n",
    "\n",
    "TAGGER_MODEL = 'polish-herbert-base'\n",
    "HYPHENATION_MODEL = 'pl_PL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'aby', 'ach', 'acz', 'aczkolwiek', 'aj', 'albo', 'ale', 'alez', 'ależ']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/other/polish_stopwords.txt', 'r') as f:\\\n",
    "    polish_stopwords = f.read().split('\\n')[:-1]\n",
    "polish_stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyglot.detect.base import logger as polyglot_logger\n",
    "polyglot_logger.setLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrases occurance calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to calculate phrase occurence coefficient (POC) in text?**\n",
    "\n",
    "1. Split by whitespace lemmatized text and phrase to separate words.\n",
    "2. Delete all stopwords and interpunction symbols from text and phrase.\n",
    "3. Enumerate all words left in text, starting from 0.\n",
    "3. For each word in phrase list all occurences (i.e. referring numbers) of the word in text. If no occurences of word in text found, then omit it (empty list).\n",
    "4. Get all possible phrase words orders in examined text i.e. perform cartesian product for positions lists.\n",
    "5. For each possible order:\n",
    "    1. Form n list of occurences into n-1 pairs.\n",
    "    2. For each pair assign (1) if first element is smaller than second (ascending order) else (-1)\n",
    "    3. Sum all assignations and divide the total by number of pairs (i.e. words in phrase - 1).\n",
    "6. Return minimum, mean and maximum score.\n",
    "\n",
    "---\n",
    "**EXAMPLE 1.**:<br />\n",
    "**text**: *Wróciły pisowskie trójki sądy doraźne koksowniki i SKOTy, do tego PiS gwałci żeby nie robić aborcji* <br />\n",
    "**phrase**: *PiS gwałci żeby nie robić aborcji*<br />\n",
    "![schema 01](charts/schemes/HSD2.0_scheme01.png)<br />\n",
    "Results: **MIN=1.0 MEAN=1.0 MAX=1.0**\n",
    "\n",
    "---\n",
    "**EXAMPLE 2.**:<br />\n",
    "**text**: *Faszystowskie sądy ach faszystowskie sądy*<br/>\n",
    "**phrase** : *Ach faszystowskie sądy fałszywe*<br />\n",
    "![schema 02](charts/schemes/HSD2.0_scheme02.png)<br />\n",
    "Results: **MIN=-0.5 MEAN=0.25 MAX=0.5**\n",
    "\n",
    "---\n",
    "**EXAMPLE 3.**:<br />\n",
    "**text**: *Ci z LGBT chcą zniszczyć pojęcia tradycji rodziny tworzonej przez mężczyznę i kobietę!*<br/>\n",
    "**phrase**: *LGBT zniszczą nową rodziny tradycję mężczyzn i kobiet.*<br />\n",
    "![schema 03](charts/schemes/HSD2.0_scheme03.png)<br />\n",
    "Results: **MIN=0.17 MEAN=0.33 MAX=0.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = COMBO.from_pretrained(TAGGER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = text.replace('#', '').replace('[...]', '')\n",
    "    sentence = tagger(text)\n",
    "    \n",
    "    lemmas = [token.lemma.lower() for token in sentence.tokens if token.deprel != 'punct']\n",
    "    lemm_text = ' '.join(lemmas)\n",
    "    \n",
    "    return lemm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POC(text, phrase, lemmatized=False, stopwords=[]):\n",
    "    \n",
    "    t = text if lemmatized else lemmatize_text(text)\n",
    "    p = phrase if lemmatized else lemmatize_text(phrase)\n",
    "    \n",
    "    t_words = list(filter(lambda x: x not in stopwords, t.split(' ')))\n",
    "    p_words = list(filter(lambda x: x not in stopwords, p.split(' ')))\n",
    "    \n",
    "    assert (len(t_words) > 0), 'The examined text must have at least one non-stopword word!'\n",
    "    \n",
    "    if len(p_words) > 1:\n",
    "        occurences = list([[i for i, x in enumerate(t_words) if x == p_w] for p_w in p_words])\n",
    "        occurences = list([o for o in occurences if len(o) > 0])\n",
    "\n",
    "        orders = list(itertools.product(*occurences))\n",
    "        order_pairs_list = list([[tuple((o[i], o[i+1])) for i, oi in enumerate(o[:-1])] for o in orders])\n",
    "\n",
    "        coeffs = list([sum([1. if op[0]<op[1] else -1. for op in ops])/(len(p_words) - 1)\n",
    "                       for ops in order_pairs_list])\n",
    "\n",
    "        return (np.min(coeffs), np.mean(coeffs), np.max(coeffs))\n",
    "    elif len(p_words) == 1:\n",
    "        return (1., 1., 1.) if p_words[0] in t_words else (0., 0., 0.)\n",
    "    else:\n",
    "        return (0., 0., 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0, 1.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Wróciły pisowskie trójki sądy doraźne koksowniki i SKOTy, do tego PiS gwałci żeby nie robić aborcji'\n",
    "phrase = 'PiS gwałci żeby nie robić aborcji'\n",
    "\n",
    "POC(text, phrase, stopwords=polish_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 0.25, 0.5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Faszystowskie sądy ach faszystowskie sądy'\n",
    "phrase = 'Ach faszystowskie sądy fałszywe'\n",
    "\n",
    "POC(text, phrase, stopwords=polish_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 0.5, 0.5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Ci z LGBT chcą zniszczyć pojęcia tradycji rodziny tworzonej przez mężczyznę i kobietę!'\n",
    "phrase = 'LGBT zniszczą nową rodziny tradycję mężczyzn i kobiet.'\n",
    "\n",
    "POC(text, phrase, stopwords=polish_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a500a772ce45eab95ff167d2f0a5e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15791.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>w czwartek muszę poprawić sądy i trybunały</td>\n",
       "      <td>w czwartek musieć poprawić sąd i trybunał</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>Żale Nałęcza i riposta Macierewicza: Pan był w kompartii, czy ma prawo wy­gła­szać takie sądy? | niezalezna.pl</td>\n",
       "      <td>żale nałęcz i riposta macierewicz pan być w kompartia czy mieć prawo wyżgłaćszać taki sąd niezalezna.pl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  \\\n",
       "0   9   \n",
       "1   8   \n",
       "\n",
       "                                                                                                             tweet  \\\n",
       "0                                                                       w czwartek muszę poprawić sądy i trybunały   \n",
       "1  Żale Nałęcza i riposta Macierewicza: Pan był w kompartii, czy ma prawo wy­gła­szać takie sądy? | niezalezna.pl    \n",
       "\n",
       "                                                                                                lemmatized  \n",
       "0                                                                w czwartek musieć poprawić sąd i trybunał  \n",
       "1  żale nałęcz i riposta macierewicz pan być w kompartia czy mieć prawo wyżgłaćszać taki sąd niezalezna.pl  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_lemmatized_tweets():\n",
    "    df = pd.read_csv(DUPLICATED_PATH)\n",
    "    df = df[['id', 'tweet']]\n",
    "    \n",
    "    if not os.path.exists(LEMMAS_PATH):\n",
    "        df['lemmatized'] = list([lemmatize_text(tweet) for tweet in tqdm(df['tweet'])])\n",
    "        df[['id', 'lemmatized']].to_csv(LEMMAS_PATH, index=False)\n",
    "    else:\n",
    "        df['lemmatized'] = pd.read_csv(LEMMAS_PATH)['lemmatized']\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = load_lemmatized_tweets()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakub/Dokumenty/venv36/lib/python3.6/site-packages/ipykernel_launcher.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "def load_lemm_phrases(load_vulg=False):\n",
    "    aphr = list([])\n",
    "    for label in LABELS_SMALL:\n",
    "        with open(HATEFUL_LEMM_DIR.replace('{}', label), 'r') as f:\n",
    "            aphr.append(np.array(f.read().split(';')))\n",
    "    if load_vulg:\n",
    "        with open(VULGARS_LEMM_DIR.replace('{}', LABELS_V_SMALL[-1]), 'r') as f:\n",
    "            aphr.append(np.array(f.read().split(';')))\n",
    "    \n",
    "    return np.array(aphr)\n",
    "\n",
    "lemm_phrases = load_lemm_phrases(load_vulg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakub/Dokumenty/venv36/lib/python3.6/site-packages/ipykernel_launcher.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "def load_ext_phrases(load_vulg=False):\n",
    "    aphr = list([])\n",
    "    for label in LABELS_SMALL:\n",
    "        with open(HATEFUL_EXT_DIR.replace('{}', label), 'r') as f:\n",
    "            aphr.append(np.array(f.read().split(';')))\n",
    "    if load_vulg:\n",
    "        with open(VULGARS_EXT_DIR.replace('{}', LABELS_V_SMALL[-1]), 'r') as f:\n",
    "            aphr.append(np.array(f.read().split(';')))\n",
    "    \n",
    "    return np.array(aphr)\n",
    "\n",
    "ext_phrases = load_ext_phrases(load_vulg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate POC score for all tweets.**\n",
    "\n",
    "1. Load relevant data with sanitized tweets and all hateful phrases.\n",
    "2. For each tweet:\n",
    "    1. For each hate type (and one vulgar):\n",
    "        1. Calculate POC scores (min, mean, max) for every phrase which belongs to certain hate type (or vulgar)\n",
    "        2. Get means of minimum, mean and maximum POC scores\n",
    "        3. Write calculations into dictionary\n",
    "    2. Write all hate types dictionary values into .csv row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a730aa5c2f4b455b8f4c43953a574669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15791.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def analyse_POC(df, phr):\n",
    "    csv_labels = list(['id'])\n",
    "    for label in LABELS_V_SMALL:\n",
    "        csv_labels.append(f'{label}_POC_min')\n",
    "        csv_labels.append(f'{label}_POC_mean')\n",
    "        csv_labels.append(f'{label}_POC_max')\n",
    "    with open(POC_SCORES_PATH, 'w') as f:\n",
    "        csv.writer(f).writerow(csv_labels)\n",
    "    del csv_labels\n",
    "\n",
    "    for _, tweet in tqdm(df.iterrows(), total=len(df)):\n",
    "        scores = dict({})\n",
    "\n",
    "        for label, phrases in zip(LABELS_V_SMALL, phr):\n",
    "            sc_min, sc_mean, sc_max = list([]), list([]), list([])\n",
    "\n",
    "            for phrase in phrases:\n",
    "                mn, mean, mx = POC(tweet['lemmatized'], phrase, lemmatized=True,\n",
    "                                   stopwords=polish_stopwords)\n",
    "                sc_min.append(mn)\n",
    "                sc_mean.append(mean)\n",
    "                sc_max.append(mx)\n",
    "\n",
    "            scores[f'{label}_min'] = np.min(sc_min)\n",
    "            scores[f'{label}_mean'] = np.mean(sc_mean)\n",
    "            scores[f'{label}_max'] = np.max(sc_max)\n",
    "            del sc_min, sc_mean, sc_max\n",
    "\n",
    "        csv_values = list([tweet['id']])\n",
    "        for label in LABELS_V_SMALL:\n",
    "            csv_values.append(scores[f'{label}_min'])\n",
    "            csv_values.append(scores[f'{label}_mean'])\n",
    "            csv_values.append(scores[f'{label}_max'])\n",
    "        with open(POC_SCORES_PATH, 'a') as f:\n",
    "            csv.writer(f).writerow(csv_values)\n",
    "        del scores, csv_values\n",
    "    \n",
    "\n",
    "if not os.path.exists(POC_SCORES_PATH):\n",
    "    analyse_POC(df, ext_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>wyz_POC_min</th>\n",
       "      <th>wyz_POC_mean</th>\n",
       "      <th>wyz_POC_max</th>\n",
       "      <th>groz_POC_min</th>\n",
       "      <th>groz_POC_mean</th>\n",
       "      <th>groz_POC_max</th>\n",
       "      <th>wyk_POC_min</th>\n",
       "      <th>wyk_POC_mean</th>\n",
       "      <th>wyk_POC_max</th>\n",
       "      <th>...</th>\n",
       "      <th>pon_POC_max</th>\n",
       "      <th>styg_POC_min</th>\n",
       "      <th>styg_POC_mean</th>\n",
       "      <th>styg_POC_max</th>\n",
       "      <th>szan_POC_min</th>\n",
       "      <th>szan_POC_mean</th>\n",
       "      <th>szan_POC_max</th>\n",
       "      <th>vulg_POC_min</th>\n",
       "      <th>vulg_POC_mean</th>\n",
       "      <th>vulg_POC_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.002193</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.004526</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006219</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.004606</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  wyz_POC_min  wyz_POC_mean  wyz_POC_max  groz_POC_min  groz_POC_mean  \\\n",
       "0   9     0.000000      0.000000          0.0          -0.5      -0.002193   \n",
       "1   8    -0.333333      0.004526          0.5          -0.5       0.000808   \n",
       "\n",
       "   groz_POC_max  wyk_POC_min  wyk_POC_mean  wyk_POC_max  ...  pon_POC_max  \\\n",
       "0           0.5          0.0      0.000000     0.000000  ...          0.5   \n",
       "1           0.5          0.0      0.006219     0.333333  ...          0.5   \n",
       "\n",
       "   styg_POC_min  styg_POC_mean  styg_POC_max  szan_POC_min  szan_POC_mean  \\\n",
       "0          -0.5       0.000260      0.500000           0.0            0.0   \n",
       "1          -0.5      -0.004606      0.333333           0.0            0.0   \n",
       "\n",
       "   szan_POC_max  vulg_POC_min  vulg_POC_mean  vulg_POC_max  \n",
       "0           0.0           0.0            0.0           0.0  \n",
       "1           0.0           0.0            0.0           0.0  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_poc_scores = pd.read_csv(POC_SCORES_PATH)\n",
    "df_poc_scores.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hateful phrases topics detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find top 20 topic 20-words sentences for phrases of each hate type (and one vulgar).**\n",
    "\n",
    "1. For each hate type:\n",
    "    1. Get relevant extended phrases.\n",
    "    2. Fit CountVectorizer and LDA model.\n",
    "    3. Save trained model into pickle archive.\n",
    "    4. For each tweet:\n",
    "        1. Calculate POC scores of each of 20 topics appearance.\n",
    "        2. Save into .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_N_TOPICS, LDA_N_WORDS = 20, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3e76e34a4f438dbf4d9e27fc9a81a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_lda_models(phr, n_topics=10):\n",
    "    for label, phrases, in tqdm(zip(LABELS_V_SMALL, phr), total=len(LABELS_V_SMALL)):\n",
    "\n",
    "        cv = CountVectorizer(stop_words=polish_stopwords)\n",
    "        count_data = cv.fit_transform(phrases)\n",
    "\n",
    "        lda_model = LDA(n_components=n_topics, n_jobs=-1)\n",
    "        lda_model.fit(count_data)\n",
    "\n",
    "        with open(LDA_MODEL_DIR.replace('{}', label), 'wb') as f:\n",
    "            pickle.dump([lda_model, cv], f)\n",
    "\n",
    "train_lda_models(ext_phrases, n_topics=LDA_N_TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_topics(lda_model, lda_cv, n_words=10):\n",
    "    words = lda_cv.get_feature_names()\n",
    "    \n",
    "    topics = list([' '.join([words[i] for i in topic.argsort()[:-n_words - 1:-1]])\n",
    "                   for topic in lda_model.components_])\n",
    "    \n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134d847ab9bf4c2e8e6e02c7c1852c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15791.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def analyse_topic_POC(df, n_words=10):\n",
    "    csv_labels = list(['id'])\n",
    "    for label in LABELS_V_SMALL:\n",
    "        csv_labels.append(f'{label}_topic_POC_min')\n",
    "        csv_labels.append(f'{label}_topic_POC_mean')\n",
    "        csv_labels.append(f'{label}_topic_POC_max')\n",
    "    with open(TOPIC_POC_SCORES_PATH, 'w') as f:\n",
    "        csv.writer(f).writerow(csv_labels)\n",
    "    del csv_labels\n",
    "\n",
    "    for _, tweet in tqdm(df.iterrows(), total=len(df)):\n",
    "        scores = dict({})\n",
    "\n",
    "        for label in LABELS_V_SMALL:\n",
    "            with open(LDA_MODEL_DIR.replace('{}', label), 'rb') as f:\n",
    "                lda_model, cv = pickle.load(f)\n",
    "\n",
    "            topics = lda_topics(lda_model, cv, n_words=n_words)\n",
    "            sc_min, sc_mean, sc_max = list([]), list([]), list([])\n",
    "\n",
    "            for topic in topics:\n",
    "                mn, mean, mx = POC(tweet['lemmatized'], topic, lemmatized=True,\n",
    "                                   stopwords=polish_stopwords)\n",
    "                sc_min.append(mn)\n",
    "                sc_mean.append(mean)\n",
    "                sc_max.append(mx)\n",
    "\n",
    "            scores[f'{label}_min'] = np.min(sc_min)\n",
    "            scores[f'{label}_mean'] = np.mean(sc_mean)\n",
    "            scores[f'{label}_max'] = np.max(sc_max)\n",
    "            del sc_min, sc_mean, sc_max\n",
    "\n",
    "        csv_values = list([tweet['id']])\n",
    "        for label in LABELS_V_SMALL:\n",
    "            csv_values.append(scores[f'{label}_min'])\n",
    "            csv_values.append(scores[f'{label}_mean'])\n",
    "            csv_values.append(scores[f'{label}_max'])\n",
    "        with open(TOPIC_POC_SCORES_PATH, 'a') as f:\n",
    "            csv.writer(f).writerow(csv_values)\n",
    "        del scores, csv_values\n",
    "\n",
    "if not os.path.exists(TOPIC_POC_SCORES_PATH):\n",
    "    analyse_topic_POC(df, n_words=LDA_N_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>wyz_topic_POC_min</th>\n",
       "      <th>wyz_topic_POC_mean</th>\n",
       "      <th>wyz_topic_POC_max</th>\n",
       "      <th>groz_topic_POC_min</th>\n",
       "      <th>groz_topic_POC_mean</th>\n",
       "      <th>groz_topic_POC_max</th>\n",
       "      <th>wyk_topic_POC_min</th>\n",
       "      <th>wyk_topic_POC_mean</th>\n",
       "      <th>wyk_topic_POC_max</th>\n",
       "      <th>...</th>\n",
       "      <th>pon_topic_POC_max</th>\n",
       "      <th>styg_topic_POC_min</th>\n",
       "      <th>styg_topic_POC_mean</th>\n",
       "      <th>styg_topic_POC_max</th>\n",
       "      <th>szan_topic_POC_min</th>\n",
       "      <th>szan_topic_POC_mean</th>\n",
       "      <th>szan_topic_POC_max</th>\n",
       "      <th>vulg_topic_POC_min</th>\n",
       "      <th>vulg_topic_POC_mean</th>\n",
       "      <th>vulg_topic_POC_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.010526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.002632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.010526</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  wyz_topic_POC_min  wyz_topic_POC_mean  wyz_topic_POC_max  \\\n",
       "0   9                0.0            0.000000           0.000000   \n",
       "1   8                0.0            0.005263           0.052632   \n",
       "\n",
       "   groz_topic_POC_min  groz_topic_POC_mean  groz_topic_POC_max  \\\n",
       "0           -0.052632             0.000000            0.052632   \n",
       "1           -0.052632            -0.010526            0.000000   \n",
       "\n",
       "   wyk_topic_POC_min  wyk_topic_POC_mean  wyk_topic_POC_max  ...  \\\n",
       "0           0.000000            0.000000           0.000000  ...   \n",
       "1          -0.052632           -0.002632           0.052632  ...   \n",
       "\n",
       "   pon_topic_POC_max  styg_topic_POC_min  styg_topic_POC_mean  \\\n",
       "0           0.052632           -0.052632             0.002632   \n",
       "1           0.000000           -0.052632            -0.010526   \n",
       "\n",
       "   styg_topic_POC_max  szan_topic_POC_min  szan_topic_POC_mean  \\\n",
       "0            0.052632                 0.0                  0.0   \n",
       "1            0.052632                 0.0                  0.0   \n",
       "\n",
       "   szan_topic_POC_max  vulg_topic_POC_min  vulg_topic_POC_mean  \\\n",
       "0                 0.0                 0.0                  0.0   \n",
       "1                 0.0                 0.0                  0.0   \n",
       "\n",
       "   vulg_topic_POC_max  \n",
       "0                 0.0  \n",
       "1                 0.0  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_poc_scores = pd.read_csv(TOPIC_POC_SCORES_PATH)\n",
    "df_topic_poc_scores.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other text scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polish Polyglot sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_sentiment(text):\n",
    "    \n",
    "    # detect and delete invalid characters first\n",
    "    t = text\n",
    "    invalid = set()\n",
    "    for i, ch in enumerate(t):\n",
    "        try:\n",
    "            Text(f\"Char: {ch}\").words\n",
    "        except:\n",
    "            invalid.add(ch)\n",
    "    for ch in invalid:\n",
    "        t = t.replace(ch, '')\n",
    "    \n",
    "    t = Text(t)\n",
    "    sents = list([])\n",
    "    for w in t.words:\n",
    "        try:\n",
    "            s = w.polarity\n",
    "        except ValueError:\n",
    "            s = 0\n",
    "        sents.append(s)\n",
    "    sents = np.array(sents)\n",
    "    \n",
    "    return np.size(sents[sents==-1]), np.size(sents[sents==0]), np.size(sents[sents==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 17, 0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sentiment('Wróciły pisowskie trójki sądy doraźne koksowniki i SKOTy, do tego PiS gwałci żeby nie robić aborcji')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5, 0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sentiment('Faszystowskie sądy ach faszystowskie sądy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 12, 0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sentiment('Ci z LGBT chcą zniszczyć pojęcia tradycji rodziny tworzonej przez mężczyznę i kobietę!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characters, syllables, words counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = pyphen.Pyphen(lang=HYPHENATION_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_numbers(text):\n",
    "    num_chars = len(text.replace(' ', ''))\n",
    "    num_syllables = sum([len(dic.inserted(word).split('-')) for word in text.split(' ')])\n",
    "    num_words = len(text.split(' '))\n",
    "    num_unique_words = len(set(text.lower().split(' ')))\n",
    "    \n",
    "    return num_chars, num_syllables, num_words, num_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 33, 16, 16)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_numbers('Wróciły pisowskie trójki sądy doraźne koksowniki i SKOTy, do tego PiS gwałci żeby nie robić aborcji')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 13, 5, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_numbers('Faszystowskie sądy ach faszystowskie sądy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74, 26, 13, 13)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_numbers('Ci z LGBT chcą zniszczyć pojęcia tradycji rodziny tworzonej przez mężczyznę i kobietę!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78, 29, 16, 16)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_numbers(lemmatize_text('Wróciły pisowskie trójki sądy doraźne koksowniki i SKOTy, do tego PiS gwałci żeby nie robić aborcji'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 11, 5, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_numbers(lemmatize_text('Faszystowskie sądy ach faszystowskie sądy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74, 25, 13, 13)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_numbers(lemmatize_text('Ci z LGBT chcą zniszczyć pojęcia tradycji rodziny tworzonej przez mężczyznę i kobietę!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate above other scores for all tweets.**\n",
    "\n",
    "1. Load relevant data with sanitized tweets.\n",
    "2. For each tweet:\n",
    "    1. Remove invalid (for polyglot) characters which cause errors.\n",
    "    2. Determine how many words have which of three sentiment types.\n",
    "    3. Count characters, syllables, words and unique words.\n",
    "    2. Write all values into .csv row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63fc3ef289c424da0fd0beca712633f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15791.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def analyse_other(df):\n",
    "    csv_labels = list([\n",
    "        'id',\n",
    "        's_neg', 's_neu', 's_pos',\n",
    "        'n_chars', 'n_sylls', 'n_words', 'nu_words',\n",
    "        'nl_chars', 'nl_sylls', 'nl_words', 'nlu_words',\n",
    "    ])\n",
    "    with open(OTHER_SCORES_PATH, 'w') as f:\n",
    "        csv.writer(f).writerow(csv_labels)\n",
    "    del csv_labels\n",
    "\n",
    "    for _, tweet in tqdm(df.iterrows(), total=len(df)):\n",
    "        scores = dict({})\n",
    "\n",
    "        scores['neg'], scores['neu'], scores['pos'] = text_sentiment(tweet['tweet'])\n",
    "        scores['chars'], scores['sylls'], scores['words'], scores['u_words'] = text_numbers(tweet['tweet'])\n",
    "        scores['l_chars'], scores['l_sylls'], scores['l_words'], scores['l_u_words'] = text_numbers(tweet['lemmatized'])\n",
    "\n",
    "        csv_values = list([\n",
    "            tweet['id'],\n",
    "            scores['neg'], scores['neu'], scores['pos'],\n",
    "            scores['chars'], scores['sylls'], scores['words'], scores['u_words'],\n",
    "            scores['l_chars'], scores['l_sylls'], scores['l_words'], scores['l_u_words']\n",
    "        ])\n",
    "        with open(OTHER_SCORES_PATH, 'a') as f:\n",
    "            csv.writer(f).writerow(csv_values)\n",
    "        del scores, csv_values\n",
    "\n",
    "if not os.path.exists(OTHER_SCORES_PATH):\n",
    "    analyse_other(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>s_neg</th>\n",
       "      <th>s_neu</th>\n",
       "      <th>s_pos</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>n_sylls</th>\n",
       "      <th>n_words</th>\n",
       "      <th>nu_words</th>\n",
       "      <th>nl_chars</th>\n",
       "      <th>nl_sylls</th>\n",
       "      <th>nl_words</th>\n",
       "      <th>nlu_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>38</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>88</td>\n",
       "      <td>33</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  s_neg  s_neu  s_pos  n_chars  n_sylls  n_words  nu_words  nl_chars  \\\n",
       "0   9      0      6      1       36       15        7         7        35   \n",
       "1   8      1     18      1       94       38       18        18        88   \n",
       "\n",
       "   nl_sylls  nl_words  nlu_words  \n",
       "0        13         7          7  \n",
       "1        33        16         16  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_other_scores = pd.read_csv(OTHER_SCORES_PATH)\n",
    "df_other_scores.head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
