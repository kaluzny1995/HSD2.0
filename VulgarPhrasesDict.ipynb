{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Speech Detector 2.0\n",
    "---\n",
    "**Vulgar Phrases Dictionary**\n",
    "\n",
    "A dictionary of polish vulgar words (i.e. swear or abusive) and phrases. Extending the dictionary using polish WordNet and researching within which wider phrases do the vulgar phrases appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "from pyplwnxml import PlwnxmlParser\n",
    "import morfeusz2\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "The three data sources have been found:\n",
    "1. Polish vulgar words [Marcinmazurek.com.pl](http://marcinmazurek.com.pl/polskie-wulgaryzmy)\n",
    "2. Polish vulgar words [Github.com](https://github.com/coldner/wulgaryzmy)\n",
    "3. Polish vulgar words & phrases [Pl.wiktionary.org](https://pl.wiktionary.org/wiki/Indeks:Polski_-_Wulgaryzmy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Set 1: 413 examples.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/vulgars/polish_vulgar_words.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "vulgar1 = np.array(re.findall(r'\\'([A-Za-z0-9_\\./\\\\-]*)\\'', text))\n",
    "f'Set 1: {len(vulgar1)} examples.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Set 2: 464 examples.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/vulgars/polish_vulgar_words_github.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "vulgar2 = np.array(re.findall(r'\\\"([A-Za-z0-9_\\./\\\\-]*)\\\"', text))\n",
    "f'Set 2: {len(vulgar2)} examples.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Set 3: 423 examples.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/vulgars/polish_vulgar_phrases.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "text = text.replace('\\n', ' • ')\n",
    "\n",
    "vulgar3 = np.array(text.split(' • '))\n",
    "f'Set 3: {len(vulgar3)} examples.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total: 846 distinct examples.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vulgar_words = np.union1d(vulgar1, vulgar2)\n",
    "vulgar_words = np.union1d(vulgar_words, vulgar3)\n",
    "f'Total: {len(vulgar_words)} distinct examples.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/vulgars/polish_vulgars.txt', 'w') as f:\n",
    "    f.write(';'.join(vulgar_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polish WordNet 3.1\n",
    "---\n",
    "Looking for synonyms of the hateful words using the polish WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_wordnet = PlwnxmlParser('models/plwordnet_3_0/plwordnet-3.0.xml').read_wordnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zły\n",
      "niedobry\n"
     ]
    }
   ],
   "source": [
    "for lu in pl_wordnet.lemma(\"zły\")[0].synsets[0].lexical_units:\n",
    "    print(lu.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piec\n",
      "piec centralny\n"
     ]
    }
   ],
   "source": [
    "for lu in pl_wordnet.lemma(\"piec\")[0].synsets[0].lexical_units:\n",
    "    print(lu.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<LexicalUnit - id:1357,name:drobiazg,pos:PartOfSpeech.NOUN,domain:Domain.ATTRIBUTE,sentiment:None>,\n",
       " <LexicalUnit - id:109778,name:drobiazg,pos:PartOfSpeech.NOUN,domain:Domain.HUMAN_PRODUCTS_NAMES,sentiment:None>,\n",
       " <LexicalUnit - id:109779,name:drobiazg,pos:PartOfSpeech.NOUN,domain:Domain.COMMUNICATION,sentiment:None>,\n",
       " <LexicalUnit - id:109781,name:drobiazg,pos:PartOfSpeech.NOUN,domain:Domain.GROUP,sentiment:None>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_wordnet.lemma(\"drobiazg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vulgar_words_ext_neg, vulgar_words_ext_neu, vulgar_words_ext_pos = list([]), list([]), list([])\n",
    "for vulgar_word in vulgar_words:\n",
    "    for lemm in pl_wordnet.lemma(vulgar_word):\n",
    "        for synset in lemm.synsets:\n",
    "            for lu in synset.lexical_units:\n",
    "                if lu.sentiment in ['- m', '- s'] and\\\n",
    "                    lu.name not in vulgar_words and lu.name not in vulgar_words_ext_neg:\n",
    "                        vulgar_words_ext_neg.append(lu.name)\n",
    "                if lu.sentiment in ['0', None] and\\\n",
    "                    lu.name not in vulgar_words and lu.name not in vulgar_words_ext_neu:\n",
    "                        vulgar_words_ext_neu.append(lu.name)\n",
    "                if lu.sentiment in ['+ m', '+ s'] and\\\n",
    "                    lu.name not in vulgar_words and lu.name not in vulgar_words_ext_pos:\n",
    "                        vulgar_words_ext_pos.append(lu.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Additional negative sentiment words: 0.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Additional negative sentiment words: {len(vulgar_words_ext_neg)}.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Additional neutral sentiment words: 453.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Additional neutral sentiment words: {len(vulgar_words_ext_neu)}.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Additional positive sentiment words: 0.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Additional positive sentiment words: {len(vulgar_words_ext_pos)}.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/vulgars/polish_additionalwn_vulgars.txt', 'w') as f:\n",
    "    f.write(';'.join(vulgar_words_ext_neu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vulgar phrases appearance analysis\n",
    "---\n",
    "Within which phrases do the vulgar words appear. Twitter scraped data. Morfeusz2 lemmatizer + Latent Dirichlet Allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First prepare Morfeusz2 lemmatizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "morf = morfeusz2.Morfeusz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mam nadzieję że dzisiejsza akcja pań z opozycji na tyle wkr Kaczora, ze dziś już policja nie będzie się certolić z tymi spędami\n",
      "0 1 ('Mam', 'mama', 'subst:pl:gen:f', ['nazwa_pospolita'], [])\n",
      "0 1 ('Mam', 'mamić', 'impt:sg:sec:imperf', [], [])\n",
      "0 1 ('Mam', 'mieć', 'fin:sg:pri:imperf', [], [])\n",
      "1 2 ('nadzieję', 'nadzieja', 'subst:sg:acc:f', ['nazwa_pospolita'], [])\n",
      "1 2 ('nadzieję', 'nadziać', 'fin:sg:pri:perf', [], [])\n",
      "2 3 ('że', 'że:c', 'comp', [], [])\n",
      "2 3 ('że', 'że:q', 'part', [], [])\n",
      "3 4 ('dzisiejsza', 'dzisiejszy', 'adj:sg:nom.voc:f:pos', [], [])\n",
      "4 5 ('akcja', 'akcja', 'subst:sg:nom:f', ['nazwa_pospolita'], [])\n",
      "5 6 ('pań', 'pani', 'subst:pl:gen:f', ['nazwa_pospolita'], [])\n",
      "6 7 ('z', 'z:p', 'prep:gen:nwok', [], [])\n",
      "6 7 ('z', 'z:p', 'prep:inst:nwok', [], [])\n",
      "6 7 ('z', 'z:q', 'part:nwok', [], [])\n",
      "7 8 ('opozycji', 'opozycja', 'subst:sg:gen:f', ['nazwa_pospolita'], [])\n",
      "7 8 ('opozycji', 'opozycja', 'subst:sg:dat.loc:f', ['nazwa_pospolita'], [])\n",
      "7 8 ('opozycji', 'opozycja', 'subst:pl:gen:f', ['nazwa_pospolita'], ['hom.'])\n",
      "8 9 ('na', 'na:i', 'interj', [], [])\n",
      "8 9 ('na', 'na:p', 'prep:acc', [], [])\n",
      "8 9 ('na', 'na:p', 'prep:loc', [], [])\n",
      "9 10 ('tyle', 'tył', 'subst:sg:loc:m3', ['nazwa_pospolita'], [])\n",
      "9 10 ('tyle', 'tył', 'subst:sg:voc:m3', ['nazwa_pospolita'], [])\n",
      "9 10 ('tyle', 'tyli', 'adj:pl:acc:m2.m3.f.n:pos', [], ['gwar.'])\n",
      "9 10 ('tyle', 'tyli', 'adj:pl:nom.voc:m2.m3.f.n:pos', [], ['gwar.'])\n",
      "9 10 ('tyle', 'tyli', 'adj:sg:acc:n:pos', [], ['gwar.'])\n",
      "9 10 ('tyle', 'tyli', 'adj:sg:nom.voc:n:pos', [], ['gwar.'])\n",
      "9 10 ('tyle', 'tyle:n1', 'num:pl:nom.acc.voc:m2.m3.f.n:rec', [], [])\n",
      "9 10 ('tyle', 'tyle:n2', 'num:sg:nom.gen.acc:m1.m2.m3.f.n:rec', [], ['rzad.'])\n",
      "9 10 ('tyle', 'tyle:d', 'adv', [], [])\n",
      "10 11 ('wkr', 'WKR', 'subst:sg:nom.acc:m3', ['nazwa_pospolita'], [])\n",
      "11 12 ('Kaczora', 'kaczor', 'subst:sg:gen.acc:m2', ['nazwa_pospolita'], ['zool.'])\n",
      "11 12 ('Kaczora', 'Kaczor:s1', 'subst:sg:gen.acc:m1', ['nazwisko'], [])\n",
      "11 12 ('Kaczora', 'Kaczór:s1', 'subst:sg:gen.acc:m1', ['nazwisko'], [])\n",
      "12 13 (',', ',', 'interp', [], [])\n",
      "13 14 ('ze', 'z:q', 'part:wok', [], [])\n",
      "13 14 ('ze', 'z:p', 'prep:gen:wok', [], [])\n",
      "13 14 ('ze', 'z:p', 'prep:inst:wok', [], [])\n",
      "14 15 ('dziś', 'dziś:s', 'subst:sg.pl:nom.gen.dat.acc.inst.loc.voc:n:ncol', ['nazwa_pospolita'], [])\n",
      "14 15 ('dziś', 'dziś:d', 'adv', [], [])\n",
      "15 16 ('już', 'już', 'part', [], [])\n",
      "16 17 ('policja', 'policja', 'subst:sg:nom:f', ['nazwa_pospolita'], [])\n",
      "17 18 ('nie', 'nie:i', 'interj', [], [])\n",
      "17 18 ('nie', 'nie:j', 'conj', [], [])\n",
      "17 18 ('nie', 'on:o', 'ppron3:pl:acc:m2.m3.f.n:ter:akc.nakc:praep', [], [])\n",
      "17 18 ('nie', 'on:o', 'ppron3:sg:acc:n:ter:akc.nakc:praep', [], [])\n",
      "17 18 ('nie', 'nie:q', 'part', [], [])\n",
      "18 19 ('będzie', 'być', 'bedzie:sg:ter:imperf', [], [])\n",
      "19 20 ('się', 'się', 'part', [], [])\n",
      "20 21 ('certolić', 'certolić', 'inf:imperf', [], [])\n",
      "21 22 ('z', 'z:p', 'prep:gen:nwok', [], [])\n",
      "21 22 ('z', 'z:p', 'prep:inst:nwok', [], [])\n",
      "21 22 ('z', 'z:q', 'part:nwok', [], [])\n",
      "22 23 ('tymi', 'ten', 'adj:pl:inst:m1.m2.m3.f.n:pos', [], [])\n",
      "23 24 ('spędami', 'spęd', 'subst:pl:inst:m3', ['nazwa_pospolita'], [])\n"
     ]
    }
   ],
   "source": [
    "text = ' Mam nadzieję że dzisiejsza akcja pań z opozycji na tyle wkr Kaczora, ' +\\\n",
    "       'ze dziś już policja nie będzie się certolić z tymi spędami'\n",
    "print(text)\n",
    "analysis = morf.analyse(text)\n",
    "for i, j, interpretation in analysis:\n",
    "    print(i, j, interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    analysis = morf.analyse(text)\n",
    "    lemmas = list([])\n",
    "    \n",
    "    i, j, interp = analysis[0]\n",
    "    last_ij, last_lemma = (i, j), interp[1].split(':')[0]\n",
    "    lemmas.append(last_lemma)\n",
    "    \n",
    "    for i, j, interp in analysis[1:]:\n",
    "        lemma = interp[1].split(':')[0]\n",
    "        if not (last_ij == (i, j) and last_lemma == lemma):\n",
    "            lemmas.append(lemma)\n",
    "        \n",
    "        last_ij = (i, j)\n",
    "        last_lemma = lemma\n",
    "    \n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mam nadzieję że dzisiejsza akcja pań z opozycji na tyle wkr Kaczora, ze dziś już policja nie będzie się certolić z tymi spędami\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mama mamić mieć nadzieja nadziać że dzisiejszy akcja pani z opozycja na tył tyli tyle WKR kaczor Kaczor Kaczór , z dziś już policja nie on nie być się certolić z ten spęd'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(text)\n",
    "lemmatize_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second combine all vulgar pure texts and lemmatized texts into single columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, texts_lemmatized = np.array([]), np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2dd6e89288d49d9be6ffcbc4d791d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1279.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(os.listdir('data/vulgars__texts')):\n",
    "    with open(f'data/vulgars__texts/{file}', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)\n",
    "        t = np.array([l for l in reader]).flatten()\n",
    "        t_l = np.array([lemmatize_text(l) for l in t])\n",
    "        \n",
    "        texts = np.concatenate([texts, t])\n",
    "        texts_lemmatized = np.concatenate([texts_lemmatized, t_l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Nie rozumiem jednego . Na chuj Glodziowi potrzebna jest taka gabarytowo kobyła . Na bank papa Francisco wypieprzy go ze stanu duchownego za tuszowanie przestępstw seksualnych . Psy z ABW już węszą skąd miał kesz na te landarę.',\n",
       "       'Nożeż qrwa nagła jej mać! Kiedy wreszcie wypieprzy z gwizdem tę imbecylkę Olejnik?!!!!! Zaprosiła (pozwoliła zaprosić) do swojej cipkokropki faszystę Krzysztofa JakaPięknaŁuna Bosaka i pozwala mu świrdolić, że wczoraj w Warszawie maszerowało \"kilka tysięcy patriotów\". '],\n",
       "      dtype='<U292')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nie on nie rozumieć jedno jeden . na chuj Glodziowi potrzebny być taki taka gabarytowo kobyła . na bank papa papać Francisco wypieprzyć go on z stan duchowny za tuszować przestępstwo seksualny . pies z ABW już węszyć skąd miał mieć kesz na te ten ty landara .',\n",
       "       'Nożeż qrwa nagły jej on mać ! Kieda kiedy wreszcie wypieprzyć z gwizd ten imbecylkę Olejnik ? ! ! ! ! ! zaprosić ( pozwolić zaprosić ) do swój cipkokropki faszysta Krzysztofa Krzysztof JakaPięknaŁuna bosak Bosak bosak i pozwalać mu on świrdolić , że wczoraj w warszawa Warszawa maszerować \" kilka tysiąc patriota \" .'],\n",
       "      dtype='<U653')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_lemmatized[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Third perform LDA algorithm and see results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'aby', 'ach', 'acz', 'aczkolwiek', 'aj', 'albo', 'ale', 'alez', 'ależ']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/other/polish_stopwords.txt', 'r') as f:\\\n",
    "    polish_stopwords = f.read().split('\\n')[:-1]\n",
    "polish_stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words=polish_stopwords)\n",
    "count_data = cv.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_topics(lda_model, lda_cv, n_words):\n",
    "    words = lda_cv.get_feature_names()\n",
    "    \n",
    "    topics = list([' '.join([words[i] for i in topic.argsort()[:-n_words - 1:-1]])\n",
    "                   for topic in lda_model.components_])\n",
    "    \n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOPICS = 10\n",
    "N_WORDS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(n_components=N_TOPICS, n_jobs=-1)\n",
    "lda.fit(count_data)\n",
    "with open('models/lda/lda_vulg.pkl', 'wb') as f:\n",
    "    pickle.dump([lda, cv], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['chuj kurwa chuja jesteś chyba dupe wie masz tyle czas',\n",
       " 'kurwa xd wiem gt chyba pis chce chuja proszę walić',\n",
       " 'robić kurwa xd chyba chce wiem serio da raz czas',\n",
       " 'xd chce mieć jebac chuj dupy wiem pis chyba ni',\n",
       " 'zrobić kobiet pis robić ludzie polaków prostu czas mamy tyłek',\n",
       " 'dupy ludzi bym jutro trzy zaraz chyba xd takich strasz',\n",
       " 'ludzie dupie łeb pis chce wrzód chodzi zamiast ludzi takim',\n",
       " 'kurwa dupy chuj chce ludzie wiem zaraz mac domu kurwie',\n",
       " 'kurwy będziecie dupę masz kurwa dupe powoli ludzi chyba potem',\n",
       " 'kurwa jesteś tez tyle 13 chyba piątek ludzi dalej wiem']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Topics found via LDA:\")\n",
    "lda_topics(lda, cv, n_words=N_WORDS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
